{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p> The research group led by professor Helton Maia (PI) from the School of Science and Technology (UFRN/ECT) at the Federal University of Rio Grande do Norte (UFRN) is at the forefront of research in Machine Learning and Computer Vision. Our team of researchers is deeply committed to pushing the boundaries of these fields, constantly exploring innovative techniques and algorithms to address real-world challenges.     </p>"},{"location":"#list-of-projetcs","title":"List of Projetcs","text":"<ul> <li>Robotic Arm Control</li> <li>Safe Distance Between Vehicles Using Drones</li> <li>Mice Tracking - PCA  </li> <li>Mice Tracking - CNN </li> <li>Artificial Intelligence for Music</li> <li>Hand Prosthesis Control</li> <li>Navigation of Robots</li> </ul>"},{"location":"#classes","title":"Classes","text":"<ul> <li>Programming Language with Python - LiP/ECT</li> <li>Computer Vision</li> <li>Deep Learning </li> </ul>"},{"location":"#books-and-chapters","title":"Books and Chapters","text":"<ul> <li>Python Book</li> <li>Frontiers in Computational Neuroscience \u2013 Editors\u2019 Pick 2021</li> <li>Object Recognition Using Convolutional Neural Networks</li> <li>Artificial Neural Networks and Efficient Optimization Techniques for Applications in Engineering</li> </ul>"},{"location":"#extra-content","title":"Extra Content","text":"<ul> <li>URA (One robot per student)</li> </ul>"},{"location":"#publications","title":"Publications","text":"<p>(doi) MENEZES, RICHARDSON ; MAIA, HELTON . An Intelligent Chess Piece Detection Tool. In: Semin\u00e1rio Integrado de Software e Hardware, 2023, Brasil. Anais do L Semin\u00e1rio Integrado de Software e Hardware (SEMISH 2023), 2023. p. 60. </p> <p>(doi) MENEZES, RICHARDSON ; DE MIRANDA, ARON ; MAIA, HELTON . PyMiceTracking: An Open-Source Toolbox For Real-Time Behavioral Neuroscience Experiments. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, New Orleans. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. p. 21427-21465.</p> <p>(doi) SILVA FILHO, D. C. ; MEDEIROS, R. A. C. ; Maia, H. . Classifica\u00e7\u00e3o de Caracteres Manuscritos para Corre\u00e7\u00e3o Autom\u00e1tica do Sistema Multiprova. In: X Escola Regional de Inform\u00e1tica de Goi\u00e1s (ERI-GO), 2022. 2022: Anais da X Escola Regional de Inform\u00e1tica de Goi\u00e1s, 2022. p. 141-152.</p> <p>(doi) MENEZES, R. S. T. ; CORDEIRO, A. M. ; MAGALHAES, R. M. ; Maia, H. . Classification of Paintings Authorship Using Convolutional Neural Network. In: XV Brazilian Congress on Computational Intelligence (CBIC), 2021, Joinville, Santa Catarina. Anais do XV Congresso Brasileiro de Intelig\u00eancia Computacional, 2021.</p> <p>(doi) FREITAS, V. Y. F. ; MENEZES, R. S. T. ; VIDAL, F. ; Maia, H. . Estimation of Safety Distance Between Vehicles on Highways Using YOLOv4 from Aerial Images. In: XV Brazilian Congress on Computational Intelligence (CBIC), 2021, Joinville, Santa Catarina. Anais do XV Congresso Brasileiro de Intelig\u00eancia Computacional, 2021. </p> <p>(doi) PEIXOTO, HELTON M.; CRUZ, ROSSANA M. S. ; MOULIN, THIAGO C. ; LE\u00c3O, RICHARDSON N. . Modeling the Effect of Temperature on Membrane Response of Light Stimulation in Optogenetically-Targeted Neurons. Frontiers in Computational Neuroscience, v. 14, p. 1, 2020. </p> <p>(doi) BASTOS, MATEUS ELOI DA SILVA ; FREITAS, VICTOR YESO FIDELIS ; DE MENEZES, RICHARDSON SANTIAGO TELES ; MAIA, HELTON . Vehicle Speed Detection and Safety Distance Estimation Using Aerial Images of Brazilian Highways. In: Semin\u00e1rio Integrado de Software e Hardware, 2020, Brasil. Anais do Semin\u00e1rio Integrado de Software e Hardware (SEMISH 2020), 2020. p. 258.</p> <p>(doi) DE MENEZES, RICHARDSON SANTIAGO TELES ; LUIZ, JOHN VICTOR ALVES ; HENRIQUE-ALVES, ARON MIRANDA ; SANTA CRUZ, ROSSANA MORENO ; MAIA, HELTON . Mice Tracking Using The YOLO Algorithm. In: Semin\u00e1rio Integrado de Software e Hardware, 2020, Brasil. Anais do Semin\u00e1rio Integrado de Software e Hardware (SEMISH 2020), 2020. p. 162.</p> <p>(doi) MENEZES, RICHARDSON SANTIAGO TELES DE ; LIMA, LUCAS DE AZEVEDO ; SANTANA, ORIVALDO ; HENRIQUES-ALVES, ARON MIRANDA ; CRUZ, ROSSANA MORENO SANTA ; MAIA, HELTON . Classification of Mice Head Orientation Using Support Vector Machine and Histogram of Oriented Gradients Features. In: 2018 International Joint Conference on Neural Networks (IJCNN), 2018, Rio de Janeiro. 2018 International Joint Conference on Neural Networks (IJCNN), 2018. p. 1-3240. </p> <p>(doi) PONTES, HEMERSON ; DE MEDEIROS, GILVANDRO ; BORGES, JOANDERSON ; MAIA, HELTON . Sistema de Computa\u00e7\u00e3o Paralela e Distribu\u00edda Utilizando Raspberry Pi e Apache Hadoop. In: Escola Potiguar de Computa\u00e7\u00e3o e suas Aplica\u00e7\u00f5es, 2018, Brasil. Anais da Escola Potiguar de Computa\u00e7\u00e3o e suas Aplica\u00e7\u00f5es (EPOCA 2018), 2018. p. 124. </p> <p>(doi) MIKULOVIC, SANJA ; PUPE, STEFANO ; PEIXOTO, HELTON MAIA ; DO NASCIMENTO, GEORGE C. ; KULLANDER, KLAS ; TORT, ADRIANO B. L. ; LE\u00c3O, RICHARDSON N. . On the photovoltaic effect in local field potential recordings. Neurophotonics, v. 3, p. 015002, 2016. </p> <p>(doi) PEIXOTO, HELTON M; MUNGUBA, HERMANY ; CRUZ, ROSSANA MS ; GUERREIRO, ANA MG ; LEAO, RICHARDSON N . Automatic tracking of cells for video microscopy in patch clamp experiments. Biomedical Engineering Online (Online), v. 13, p. 78, 2014. </p> <p>(doi) PEIXOTO, H. M.; DINIZ, A. A. R. ; ALMEIDA, N. C. ; DE MELO, J. D. ; DORIA NETO, A. D. ; GUERREIRO, A. M. G. . Modeling a system for monitoring an object using artificial neural networks and reinforcement learning. In: 2011 International Joint Conference on Neural Networks (IJCNN 2011 San Jose), 2011, San Jose. The 2011 International Joint Conference on Neural Networks. v. 01. p. 2327-2332. </p> <p>(doi) PEIXOTO, H. M.; GUERREIRO, A. M. G. ; DORIA NETO, A. D. . Image Processing for Eye Detection and Classification of the Gaze Direction. In: The 2009 International Joint Conference on Neural Networks, 2009, Atlanta. IJCNN 2009 Conference Proceedings, 2009. p. 2475-2480.</p>"},{"location":"projects/roboticarm/","title":"Roboticarm","text":"<p>Project developed at the Automation and Robotics Laboratory (LAR) of the School of Science and Technology (ECT) of the Federal University of Rio Grande do Norte (UFRN)</p> <p>Current project members: - Kennymar Bezera de Oliveira - Thiago Vinicius Cardoso Lopes</p> <p>Supervisor: Helton Maia </p>"},{"location":"projects/roboticarm/#robotic-arm-control-using-computer-vision-and-machine-learning","title":"Robotic Arm Control Using Computer Vision and Machine Learning","text":"<p>This project encompasses computer vision, machine learning, 3D printing and integrates with Arduino, with the main objective of enabling robotic arms to reproduce the movements of a human arm. The project involves a series of technologies using the OpenCV computer vision library, such as capturing images in real time using a webcam or smartphone. Then, detecting relevant information from the user's hand through our neural network trained based on the YOLOv8 architecture. Finally, the control of the robotic arm, which was done via 3D printing, is carried out by Arduino according to serial port commands given by the python script using the Pyserial module. This control is based on an intelligent decision-making system that links the user's hand movements to the robotic arm.</p> <p>Project video</p>"},{"location":"projects/roboticarm/#using-the-project","title":"Using the project","text":""},{"location":"projects/roboticarm/#requirements","title":"Requirements","text":"<ul> <li>Anaconda package (Install Guide) or any IDE of your choice</li> <li>Python 3.10 (Install Guide) (If you choose to use an IDE of your choice)</li> <li>OpenCV</li> <li>Ultralytics</li> <li>Pyserial</li> <li>ipykernel</li> <li>Webcam or a smartphone</li> </ul>"},{"location":"projects/roboticarm/#creating-a-virtual-environment","title":"Creating a virtual environment","text":"<p>In this part, the basic configuration necessary for the implementation of the algorithm is described according to the specified requirements.</p> <p>Assuming that the installation of the Anaconda package (or your favorite IDE) has already been done, it is time to create a virtual environment, which will provide the installation of only the necessary modules and an independent environment for the development and execution of the project using specific versions according to the need.</p> <p>If you are using an IDE of your choice, just make sure to set up your virtual environment using python 3.10 version. In this tutorial, we will just show how to use the Conda virtual environment manager, which is already included in the Anaconda package.</p> <p>Open the Anaconda prompt and follow the command changing MyEnvironmentName to the desired name for your environment.</p> <pre><code>conda create --name MyEnvironmentName python=3.10\n</code></pre> <p>With the environment now created, activate it changing MyEnvironmentName to the name you gave it.</p> <pre><code>conda activate MyEnvironmentName\n</code></pre>"},{"location":"projects/roboticarm/#installing-modules-and-registering-the-environment","title":"Installing modules and registering the environment","text":"<p>Here we need to install the necessary packages listed in the requirements section. Keep in mind that they will only be installed in your virtual environment, which is currently activated.</p> <p>This part also applies if you are using the IDE of your choice and another virtual environment manager. </p> <p>Now install the packages one by one using the following commands.</p> <pre><code>pip install opencv-python\n</code></pre> <pre><code>pip install ultralytics\n</code></pre> <pre><code>pip install pyserial\n</code></pre> <pre><code>pip install ipykernel\n</code></pre> <p>In case you are using the Anaconda package, you need to register your conda virtual environment using the ipykernel module. If you don't do this, the Jupyter Notebook, that was installed within the Anaconda package, will not recognize your virtual environment and the modules on it. Once again change MyEnvironmentName to the name you gave it</p> <pre><code>python -m ipykernel install --user --name=MyEnvironmentName\n</code></pre>"},{"location":"projects/roboticarm/#accessing-the-camera","title":"Accessing the camera","text":"<p>If you don't have a webcam, you can use the Droidcam app on a smartphone to use the phone's camera as a webcam</p> <ul> <li>Install the Droidcam app (Android or IOS): Download Page</li> <li>Install the Droidcam client on Windows: Download Page</li> <li>Then follow the instructions to connect: Connection Guide</li> </ul>"},{"location":"projects/roboticarm/#detecting-the-hand","title":"Detecting the hand","text":"<p>To use the main algorithm, it is necessary to ensure the correct hand detection using our neural network.</p> <p>Once you have access to a camera, use Jupyter Notebook to open the file \"test-hand-detection.py\" available in this project. </p> <p>Look for the \"Kernel\" tab above the code within Jupyter Notebook editor, and then change the kernel to your virtual environment's one.</p> <p>Download our \"weight-hand-segmentation-v11.pt\" file, available in the weights folder of this project, and put it in the same directory as the \"test-hand-detection.py\" script. </p> <p>With our weights file, you will be able to use our trained neural network to detect and segment any hand in the camera feed.</p> <p>Now you can run the script and see if the hand is detected and segmented correctly as shown below.</p> <p></p> <p>After the correct detection of the hand it is necessary to close the jupyter notebook (or the IDE that you are using) and deactivate the virtual environment, and only after the Arduino part is done be activated again, otherwise both codes will try to take control of the serial port and the serial commands comming from the python script will not be read. The next access to the Anaconda prompt and the Jupyter Notebook should be via the Anaconda Navigator, after running it as administrator and while the Arduino script is running.</p> <p>You can deactivate your virtual environment typing in the Anaconda prompt</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"projects/roboticarm/#configuring-the-arduino-ide","title":"Configuring the Arduino IDE","text":"<p>Once you installed the Arduino IDE, you can download the zip file of the VarSpeedServo library available here and install it by following this guide. Now use the file \"serial-port-command-receiver.cpp\" in the Arduino IDE.</p>"},{"location":"projects/roboticarm/#assembling-the-circuit","title":"Assembling the circuit","text":"<p>The following circuit was simulated using the Wokwi online platform, and considering that the four servomotors are already fitted to the robotic arm, the circuit of this image bellow must be physically assembled.</p> <p></p> <p>If you want to save time, use our pin configuration as shown in the circuit:</p> <pre><code>Gripper -&gt; Pin 2\nLower Arm -&gt; Pin 3\nUpper Arm -&gt; Pin 4\nRotating Base -&gt; Pin 5\n</code></pre> <p>If you want to change the pins, just make sure the digital pins on the Arduino board receiving the signal jumpers from the servomotors are constant values specified in the file \"serial-port-command-receiver.cpp\" as \"#define something_pin\".</p> <p>For example, if the servomotor that controls the robotic arm's gripper is connected to digital pin 2 on the Arduino board, the definition in the code should be \"#define gripper_pin 2\".</p> <p>Select the right serial port within the IDE, compile and send the code to the board.</p>"},{"location":"projects/roboticarm/#using-the-main-algorithm","title":"Using the main algorithm","text":"<p>While the Arduino script is running, run Anaconda Navigator as administrator and launch the Anaconda prompt. Running as admin also applies if you are using another IDE.</p> <p>Now activate your virtual environment, go back to the Navigator interface and launch Jupyter Notebook. Don't forget to activate your Kernel again.</p> <p>Download the following files and put them in the same directory as the \"weight-hand-segmentation-v11.pt\" file.</p> <pre><code>main.py\nrobotic_arm.py\ndetection_infos.py\ninterface.py\n</code></pre> <p>Locate the COM variable in the \"main.py\" file and change it to the serial port currently connected to the arduino (check the Arduino IDE in case you don't remember).</p> <p>For example, if your port is COM3, you need to set the COM variable to 3 and so on.</p> <p>Now you can run the code and control the robotic arm by the movements of your hand.</p> <p>To facilitate the process of acquiring the files, you can also download this entire project.</p>"},{"location":"projects/roboticarm/#examples","title":"Examples","text":"<p>The following images are examples that show what approximate positions the hand must be in for detections to be made correctly.</p> Open hand <p></p> Closed hand <p></p>"}]}